


Appel à soumission : numéro spécial de la revue « Document Numérique »

              « Évaluation en Recherche d'Information »

---------------------------------- Calendrier ----------------------------------
Date limite de soumission : 27 juin 15 juin
Notification aux auteurs : 15 septembre
Réception des versions finales : 15 octobre
--------------------------------------------------------------------------------


Objectifs

L'évaluation en Recherche d'Information a une longue histoire. La
tradition expérimentale dans ce domaine est très fortement présente
depuis les travaux de Cleverdon dans les années soixante. De ces
travaux sont apparus le besoin de collections qui permettent de
travailler sur des données partagées par la communauté et des mesures
d'évaluation définies selon la tâche à accomplir.

De très nombreuses campagnes d'évaluation ont suivi les travaux de
Cleverdon en proposant des collections de volume toujours plus
important et pour des tâches de plus en plus diversifiées.  D'abord en
langue anglaise, la diversification s'est d'abord orientée vers
d'autres langues, puis vers des données au delà du texte : hyperliens,
structuration des documents, données sociales, ou encore par l'accès à
des ressources externes aux documents telles que des ontologies.

Outre les documents, les besoins d'informations doivent être créés en
accord avec la tâche à accomplir. La première étape est donc la
définition aussi précise que possible de la tâche, puis la construction
et/ou la collecte de ces besoins.

Associé à la collecte de documents, il y a la construction des
jugements. Pour la tâche ad'hoc de base, ces jugements indiquent quels
sont les documents pertinents pour tel ou tel besoin d'information,
avec toute la difficulté associée à cette notion de pertinence. Pour
d'autres tâches ce sont d'autres qualifications des réponses possibles
qui sont utiles ou d'autres niveaux de granularité pour la
pertinence.

Du côté des mesures, pour la tâche ad'hoc les mesures prédominantes et
largement acceptées aujourd'hui utilisent les notions de précision et
de rappel. Il existe cependant de très nombreuses mesures sur cette
base et des variations de ces mesures pour tenter de coller à telle
ou telle particularité de telle ou telle tâche.

Enfin, la mise en oeuvre de ces mesures pour l'évaluation de tel ou
tel système et plus particulièrement concernant l'amélioration des
performances nécessite un certain soin. En effet la différence
purement quantitative n'est pas nécessairement la preuve absolue d'un
gain réel. Pour cela des tests statistiques sont aussi mis en oeuvre
lors des comparaisons de systèmes.

Dans ce numéro nous invitons la soumission d'articles sur les thèmes
suivants (liste non exhaustive) :

. Modèle de collection de documents
. Critères de construction/utilisation de collection de documents
. Problèmes d'anonymisation dans les données
. Problèmes de droits d'utilisation

. Définition des tâches
. Construction/collecte de besoin d'information
. Notion de qualité des réponses : pertinence, utilité,...
. Construction des jugements : méthodes, outils

. Mesures d'effectivité
. Mesures d'efficience
. Limites des mesures d'évaluation
. L'évaluation de tâches spécifiques : interaction, retour de pertinence,...
. L'évaluation de nouvelles tâches
. Tests statistiques

Recommandations aux auteurs

Outre des contributions originales telles que la proposition d'une
nouvelle mesure ou la construction d'une nouvelle collection par
exemple, des articles qui font un large panorama par exemple sur des
collections ou des mesures existantes sont aussi les bienvenus.

Faire une déclaration d'intention de soumission aussitôt que possible.

Les contributions doivent être significatives et originales (en français
ou anglais pour les auteurs non francophones).
Elles doivent respecter le format de la revue Document Numérique
(http://dn.revuesonline.com/)
Elles doivent avoir une taille de 20 à 25 pages.
Elles doivent parvenir sous format PDF au plus tard le 27 juin 15 juin 2014 à:
      michel.beigbeder@emse.fr
      sylvie.calabretto@insa-lyon.fr
      max.chevalier@irit.fr


Co-éditeurs

Michel Beigbeder, Ecole nationale supérieure des mines de Saint-Étienne
Sylvie Calabretto, LIRIS, INSA de Lyon
Max Chevalier, IRIT, Université Paul Sabatier, Toulouse


Comité de lecture

En cours de constitution.

