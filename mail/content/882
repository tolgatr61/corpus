Numéro spécial de la revue TAL "Apprentissage profond pour le traitement
automatique des langues"
2nd Call for paper, special issue of TAL on "Deep Learning for natural
language processing"
tal-59-2.sciencesconf.org
(see English version bellow)


"Apprentissage profond pour le traitement automatique des langues" /
"Deep Learning for natural language processing"

Ces dernières décennies, les réseaux de neurones artificiels et plus
généralement l'apprentissage profond de ces modèles (deep-learning) ont
renouvelé les perspectives de recherche en traitement automatique des
langues (TAL). La plupart des applications en TAL (e.g analyse
syntaxique et sémantique des textes et du discours, résumé et traduction
automatique, ...) nécessitent de modéliser des données structurées
(séquences, arbres, graphes, ...) qui se caractérisent par des
distributions particulières, parcimonieuses et avec des espaces de
réalisations de grande dimension. Dans ce contexte, les réseaux de
neurones ont permis des avancées importantes en ce qui concerne les
représentations continues pour le TAL et ce pour de nombreuses tâches
comme par exemple l'analyse syntaxique, la classification de document,
la reconnaissance automatique de la parole et la traduction automatique.
Ces progrès se sont amplifiés dès lors que les modèles neuronaux ont
dépassé le cadre de l'apprentissage de représentation pour évoluer vers
des architectures de plus en plus profondes, permettant de modéliser de
bout en bout des tâches d'inférence complexes.

Néanmoins, ces évolutions récentes posent de nombreuses questions
scientifiques. Si les performances obtenues par les modèles neuronaux
impressionnent souvent, les architectures déployées sont complexes à
concevoir et à optimiser. La vision de ces modèles comme une boîte noire
est problématique tant l'interprétation des résultats et la
compréhension de ce qui est appris restent obscures.

L'objectif de ce numéro numéro spécial de la revue TAL
(http://www.atala.org/-Revue-TAL) est de proposer une vue d’ensembledes
recherches actuelles portant sur le deep-learning pour le TAL, d'en
montrer à la fois les promesses, les limites et en quoi les
représentations obtenues diffèrent ou au contraire ressemblent à
d’autres représentations plus anciennes en TAL.

Les propositions de communications pourront notamment porter sur les
thèmes suivants :
- application des réseaux neuronaux au TAL et à la linguistique
- les architectures neuronales pour le TAL
- apprentissage de représentation, des caractères aux documentsen
passant par les mots
- interprétation et explication des représentations apprises
- inférence structurée et modèle de génération pour le TAL
- apprentissage et optimisation dans le cas spécifique du TAL

Les soumissions doivent correspondre à des articles complets non déjà
publiés ailleurs et portant explicitement sur des applications du
Deep-Learning au TAL. De plus, il est important que les articlessoumis
soient accessibles à la communauté TAL. Donc si des aspects
méthodologiques liés aux réseaux de neurones sont abordés, il est
important qu'un effort pédagogique soit fait afin de les rendre
accessibles à la communauté.

DATES

- Date limite de soumission: 31 mars 2018
- Notification aux auteurs, première relecture : 31 mai 2018
- Notification aux auteurs, seconde relecture : 16 juillet 2018
- Publication: Octobre 2018

LANGUE

Les articles sont écrits en français ou en anglais. Les soumissions en
anglais ne sont acceptées qu’en cas de présence d’au moins un auteur non
francophone.

LA REVUE

La revue TAL (Traitement Automatique des Langues) est une revue
internationale éditée depuis 1960 par l’ATALA (Association pour le
Traitement Automatique des Langues) avec le concours du CNRS. Elle est
maintenant publiée en format électronique, avec accès gratuit immédiat
aux articles publiés, et impression annuelle à la demande. Celane
change aucunement son processus de relecture et de sélection.

FORMAT

Les articles doivent faire entre 20 et 25 pages. Les auteurs doivent
contacter les rédacteurs pour obtenir une dérogation sur la
longueur. Les chercheurs ayant l’intention de soumettre une contribution
sont invités à déposer leur article en cliquant sur le menu "Soumission
d’un article" (format PDF). Pour cela, si ce n’est déjà fait, s’inscrire
sur le site http://www.sciencesconf.org (en haut à gauche, "créer un
compte"), puis revenir sur la page http://tal-59-2.sciencesconf.org/, se
connecter et effectuer le dépôt.

La revue TAL a un processus de relecture en double-aveugle. Merci
d’anonymiser votre article et le nom du fichier.

Les feuilles de style sont disponibles en ligne sur le site de la revue
(http://www.atala.org/Instructions-aux-auteurs-feuilles).


COMITÉ SCIENTIFIQUE

Rédacteurs invités:
Alexandre Allauzen (Université Paris-Sud / LIMSI-CNRS)
Hinrich Schütze (LMU)

Membres:
Marianna Apidianaki, LIMSI-CNRS
Loic Barrault, Université du Maine, LIUM
Fethi Bougares, Université du Maine, LIUM
Hervé Bredin, LIMSI-CNRS
Trevor Cohn, University of Melbourne
Marta R. Costa-jussà, Universitat Politecnica de Catalunya
Benoit Crabbé, Université Paris Diderot, LLF
Richard Dufour, Université d'Avignon, LIA
Benoit Favre, Université d'Aix-Marseille, LIF
George Foster, CNRCR
Fabrice Lefevre, Université d'Avignon, LIA
Joseph Leroux, Université Paris-Nord, LIPN
Hermann Ney, RWTH Aachen University
Jan Nieheus, Karlsruhe Institute of Technology
Christian Raymond, IRISA
Holger Schwenk, Facebook
Tim Van de Cruys, IRIT

**************************************************************************
English Version
**************************************************************************


Deep Learning for natural language processing

During the last decade, artificial neural networks and deep
learning approaches have strongly renewed the research
perspectives in Natural Language Processing (NLP). Most
applications of NLP (e.g., syntactic parsing, text and
discourse analysis, summarization and translation) require
to model structured data (sequences, trees, graphs, ...)
characterized by peculiar and sparse distributions of events
along with a large set of possible outcomes. To tackle such
challenges, neural networks provide an efficient way to
introduce continuous representation of linguistic units and
to learn them jointly with the decision process, yielding
important improvement in several tasks. These tasks include
document classification, syntactic parsing, automatic speech
recognition and machine translation.  More recently, deeper
neural architectures have been successfully trained
end-to-end for hard inference tasks, in particular, for
neural machine translation.

These trends open a wide range of scientific perspectives
for NLP research. Whereas the performance achieved by neural
networks are impressive, their conception and optimization
are still challenging. Moreover, these architectures are
merely understood as efficient black boxes and their results
remain difficult to interpret and explain.

The goal of this issue of Traitement Automatique des Langues
(TAL) is to give an overview of the research on "deep
learning for NLP", its promise along with its limits and its
relationship with other approaches.

Authors are invited to submit papers on all aspects of deep
learning for NLP, in particular regarding, but not limited
to, the following issues and tasks:

- application of neural networks to NLP and linguistic studies
- neural architectures for NLP
- representation learning, from characters to documents, including words
- interpretability and explainability of representations and models
- structured inference and generative models for NLP
- learning and optimization of deep networks for NLP peculiarities

Authors are invited to submit full papers, describing original work on
applications of Deep Learning to NLP. Moreover, articles must be
accessible and understandable to the entire NLP community. Therefore,
for relevant contributions which are related to methodological or
technical aspects of deep networks, their description must be
understandable by most of the readers.

DATES

- Submission deadline: 31 of March 2018
- Notification to the authors after first review: 31 of May 2018
- Notification to the authors after second review: 16 of July 2018
- Publication: October 2018

LANGUAGE

Manuscripts may be submitted in English or French. French-speaking
authors are requested to submit their contributions in French.

JOURNAL

Traitement Automatique des Langues is an international journal
published since 1960 by ATALA (Association pour le traitement
automatique des langues) the French association for NLP with the
support of CNRS. It is now published online, with an immediate open
access to published papers, and annual print on demand. This does not
change its editorial and reviewing process.

FORMAT

Papers must be between 20 and 25 pages. Authors who intend to submit
a paper are encouraged to upload their contribution on
http://tal-59-2.sciencesconf.org/, via the menu "Paper submission"
(PDF format). To do so, they will need to have an account on the
sciencesconf platform. To create an account, go to the site
http://www.sciencesconf.org and click on "create account" next to the
"Connect" button at the top of the page. To submit, come back to the
page http://tal-59-2.sciencesconf.org/, connect to the account and
upload the submission. From now on, TAL will perform double-blind
review: it is thus necessary to anonymize the manuscript and the name
of the pdf file. Style sheets are available for download on the Web
site of the journal (http://www.atala.org/English-style-files).


SPECIAL ISSUE EDITORIAL BOARD

Guest editors:
Alexandre Allauzen (Université Paris-Sud / LIMSI-CNRS)
Hinrich Schütze (LMU)

Members:

Marianna Apidianaki, LIMSI-CNRS
Loic Barrault, Université du Maine, LIUM
Fethi Bougares, Université du Maine, LIUM
Hervé Bredin, LIMSI-CNRS
Trevor Cohn, University of Melbourne
Marta R. Costa-jussà, Universitat Politecnica de Catalunya
Benoit Crabbé, Université Paris Diderot, LLF
Richard Dufour, Université d'Avignon, LIA
Benoit Favre, Université d'Aix-Marseille, LIF
George Foster, CNRCR
Fabrice Lefevre, Université d'Avignon, LIA
Joseph Leroux, Université Paris-Nord, LIPN
Hermann Ney, RWTH Aachen University
Jan Nieheus, Karlsruhe Institute of Technology
Christian Raymond, IRISA
Holger Schwenk, Facebook
Tim Van de Cruys, IRIT

-- 
      Alexandre Allauzen
  Univ. Paris XI, LIMSI-CNRS
Tel : 01.69.85.80.64 (80.88)
Bur : 114     LIMSI Bat. 508
      allauzen@limsi.fr



