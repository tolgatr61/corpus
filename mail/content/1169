Thèse financée dans le cadre du projet ANR Ethicaa - Éthique des Agents 
Autonomes

Sujet : modélisation des systèmes éthiques


  1- Institutions d’accueil et direction de thèse

Le candidat sera accueilli au sein de l’équipe ACASA du LIP6 à 
l’Université Pierre et Marie Curie. Il travaillera en collaboration 
étroite avec des enseignants-chercheurs des écoles de l’institut Mines 
Télécom, en particulier avec Pierre-Antoine Chardel et Thibault de 
Swarte, sous la direction conjointe de Jean-Gabriel Ganascia et de 
Robert Voyer.


      Contact: Jean-Gabriel Ganascia (Jean-Gabriel.Ganascia@lip6.fr)


  2- Sujet de thèse


    Introduction

Cette thèse s’inscrit dans le cadre du projet ANR Ethicaa (Ethique des 
Agents Autonomes) qui se propose d’étudier les spécifications d’agents 
artificiels autonomes interagissant avec des humains et capables de les 
aider à résoudre des conflits éthiques. Ces conflits apparaissent à la 
fois au sein d’un agent, soumis à des principes éthiques antagoniques, 
entre les agents eux-mêmes, voire entre les agents et les opérateurs 
humains.

Ce sujet de thèse porte sur les différentes façons de surmonter les 
conflits entre principes éthiques /au sein d’un seul agent/. Pour 
cela, il faudra faire référence à différentes théories éthiques décrites 
par divers courants philosophiques (conséquentialisme, déontisme, 
etc.)en vue de les modéliser en ayant recours à différentes 
formalisations issues des travaux d’intelligence artificielle inspirés 
des logiques modales, des logiques non-monotones (comme les ASP (Gelfond 
2007)) de, l'argumentation formelle (Dung 1995)) ou de la programmation 
d’agents intelligents, comme le formalisme BDI (Rao & Georgeff, 1995).Il 
s’agira enfin de valider les différents modèles sur des études de cas 
déjà inventoriées dans le cadre du projet Ethicaa.

Ce sujet s’inscrivant à la frontière de la philosophie et de 
l’intelligence artificielle, il sera encadré à la fois par des 
philosophes et par des spécialistes d’intelligence artificielle.


    Travail à effectuer

Le travail s’effectuera en trois temps.

Dans le premier, on reprendra les différentes modélisations 
informatiques des raisonnements éthiques effectuées au cours de ces 
dernières années (Ganascia, 2007 ; 2014 ; Bringsjord & al., 2006 ; 
Power, 2005). On les mettra alors en regard des différents systèmes 
éthiques identifiés par les philosophes. On comparera ces différentes 
formalisations et on déterminera les propriétés sémantiques et 
algorithmiques de chacune d’entre elles.

Dans un second temps, on concevra un ou éventuellement deux formalismes 
de représentation des connaissances, voire plus si cela se révèle 
nécessaire puis on les déploiera sur des architectures d’agents 
intelligents. Sur la base de ces représentations l’agent devra être en 
mesure soit de réguler son comportement, soit d’évaluer et/ou de 
« juger » le comportement d’autres agents (autonomes ou humains) selon 
des critères éthiques. On s’intéressera tout particulièrement à la 
capacité de ces agents à surmonter les dilemmes éthiques, ou, le cas 
échéant, à expliciter clairement les différentes alternatives, et à 
argumenter leurs positions avec des agents, qu’ils soient humains ou 
artificiels.

On s'appuiera en particulier sur des modélisations symboliques du 
raisonnement, telles que l'argumentation formelle, l'abduction et les 
formes de programmation logique non monotones.

Dans un troisième temps, on procèdera à un implémentation informatique 
de chacun des formalismes de représentation des connaissances mentionnés 
plus haut, afin de les tester sur les cas d’étude identifiés dans le 
cadre du projet Ethicaa.

On s’intéressera plus particulièrement au /partage d’autorité/ entre 
hommes et machines dans l’aéronautique, aux /transactions à haute 
fréquence/ dans la bourse et à /l'investissement responsable/.


    Prérequis

Pour effectuer ce sujet de thèse, il faudrait avoir reçu une formation 
préalable en intelligence artificielle symbolique, avec une connaissance 
des différents formalismes de modélisation du raisonnement et de 
déduction automatique.

Il faudrait aussi être en mesure d’implémenter ces différents modèles et 
de les tester.

Enfin, il serait souhaitable demanifester un goût pour les questions 
philosophiques, et éventuellement avoir une formation dans ce domaine.


    Références

Bringsjord, S., Arkoudas, K., Bello, P.: Toward a General Logicist 
Methodology for Engineering Ethically Correct Robots. Technical report, 
Rensselaer Polytechnic  Institute (RPI), Troy NY 12180 USA (2006)

Dung., P. M. : On the acceptability of arguments and its fundamental 
role in nonmonotonic reasoning, logic programming and /n/-person games. 
/Artificial Intelligence/, 77:321–357, 1995.

Ganascia, J.G.: Modelling ethical rules of lying with answer set 
programming.  Ethics and Information Technology 9(1) (2007) 39–47

Ganascia, J.G.: An agent-based formalization for resolving ethical 
conflicts. In Konieczny, S., Meyer, T., eds.: Workshop on Belief change, 
Non-monotonic rea- soning and Conflict Resolution, Montpellier, France, 
ECAI (August 2012) 34–40

Ganascia, J.G.: Non-Monotonic Resolution of Conflicts for Ethical 
Reasoning. To appear MIT Press (2014)

Gelfond, M.: Answer sets, chapter 7. In: Handbook of Knowledge 
Representation.  Elsevier (2007)

Gensler, H.: Formal Ethics. Routledge (1996)

Powers, T.: Deontological machine ethics. Technical report, American 
Association  of Artificial Intelligence Fall Symposium 2005, Washington, 
D.C. (2005)

Rao, A., Georgeff, M.: Bdi agents: From theory to practice. Proceedings 
of the  first international conference on multiagent systems ICMAS95 
95(Technical Note  56) (1995) 312–319

Tufis, M., Ganascia, J.G.: Grafting norms onto the BDI agent model. In: 
A Construction Manual for Robot’s Ethical Systems: Requirements, 
Methods, Implementations. To appear MIT Press (2014)

vonWright, G.H.: Deontic logics. Mind (60) (1951) 1–15


**

